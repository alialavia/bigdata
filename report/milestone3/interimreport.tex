% This file is the Latex source of the Big Data course project
% report. The project contributors are Ali Alavi, Rolf jagerman
% and Ken Tsay.
% The report is written by Ali Alavi, Rolf Jagerman.

%
\documentclass{llncs}
%

\usepackage{graphicx} % for importing images
\usepackage{caption}
\usepackage{subcaption} % for subfigures
\captionsetup{compatibility=false} % to make subfigures compatible with template

\usepackage{url} % for URL references
\usepackage{float} % helps with locating the 
\usepackage[T1]{fontenc}  % providing font encoding
% used for drawing the diagrams
%
\begin{document}
%
\mainmatter              % start of the contributions
\pretolerance=10000  % This avoids long lines
\pagestyle{headings}
%\hyphenation{}

%
\title{Automatic News Generation Based on Twitter}
%
\titlerunning{Automatic News Generation Based on Twitter}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Ali Alavi\inst{1} \and Rolf Jagerman\inst{1} \and
Tsay Kai-En\inst{1}}
%
\authorrunning{Ali Alavi, Rolf Jagerman and Tsay Kai-En} % abbreviated author list (for running head)
%
%
\institute{ETH Z\"urich, Z\"urich, Switzerland\\
\email{alavis@ethz.ch, \{rolfj, tsayk\}@student.ethz.ch}
}

\maketitle              % typeset the title of the contribution
%

\section{Introduction}
% todo
...

\section{Data model}
% todo
...


\section{Design of the system}
% todo
...


\section{Results}

\subsection{Classification performance}

In our previous report, we measured our news classifier's performance by using precision, recall and f1-score. These scores are shown in table \ref{tbl:oldclassifier}.

\begin{table}
	\begin{center}
		\begin{tabular}{|r|r|r|r|r|} \hline
			class  & precision   & recall & f1-score  & support \\ \hline
			technology    &   1.00 &     0.08  &    0.14   &   6195 \\
			sports   &    1.00   &   0.03   &   0.07   &   6365 \\
			politics   &    1.00  &    0.10   &   0.18   &   6376 \\
			avg / total  &     1.00   &   0.07  &    0.13   &  18936 \\ \hline
		\end{tabular}
	\end{center}
	\caption{Old classification performance over the three trained categories}
	\label{tbl:oldclassifier}
\end{table}

It was evident that the classifier was not performing optimally. The high precision measures were offset by the extremely low recall measures. This meant that the classifier was only predicting the relevant labels for a few tweets and discarding most of them. This was likely the result of training on an imbalanced data set.

The new classifier was trained on roughly two to three times as much data. Additionally, we ensured that each classifier in the one-vs-all approach was trained on roughly equally many positive and negative datasamples. The scores for the classifier are shown in table \ref{tbl:newclassifier}

\begin{table}
	\begin{center}
		\begin{tabular}{|r|r|r|r|r|} \hline
			class  & precision   & recall & f1-score  & support \\ \hline
			technology    &   0.78 &     0.92  &    0.84   &  25343 \\
			sports   &    0.75   &   0.73   &   0.74   &   25343 \\
			politics   &    0.83  &    0.75   &   0.79   &   25343 \\
			avg / total  &     0.79   &   0.80  &    0.79   &  76029 \\ \hline
		\end{tabular}
	\end{center}
	\caption{New classification performance over the three trained categories}
	\label{tbl:newclassifier}
\end{table}

Although the precision of the classifier has gone down, our recall has increased a lot. This means that, although some predictions are wrong, we have a lot more predictions to work with.

Despite scaling up the entire system, the amount of available training data did not increase significantly. The changes in performance are not due to an increase of data, but are more likely attributed to training on a more balanced data set.

\subsection{Prediction scalability}

Our main goal in this part of the project was to scale up our prediction capability. We want to use our trained classifier to quickly classify all of the collected tweets. We were able to classify 600GB of tweets in roughly 10 hours using 20 Amazon EC2 m1.medium instances. The Spark Map-Reduce framework created 10,702 tasks. Each of these tasks independently computes the predictions on a part of the dataset and write the results to a file on s3. A single task takes on average $65.4 (\pm 20.6)$ seconds to complete. Since we were restricted to 20 EC2 instances, this takes about 10 hours in total. With sufficiently available instances, this computation could have been done in minutes instead of hours.

\section{Future Work}
% todo
...




\bibliographystyle{plain}
\bibliography{report.bib}

\end{document}